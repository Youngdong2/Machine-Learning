# Ch.02 모델 평가 및 선택

## 2.1 경험 오차 및 과적합

우리는 일반적으로 전체 샘플 수와 잘못 분류한 샘플 수의 비율을 **오차율(error rate)**라고 부른다. 만약 m개의 샘플 중에서 a개의 샘플이 잘못 분류되었다면, 오차율 E = a/m이 된다. 이와 관련하여 '**정밀도(accuracy)** = 1 - 오차율'이다. 조금 더 일반적으로는 모델의 실제 예측 값과 샘플의 실제 값 사이의 차이를 **오차(error)** 라 부르고, 모델이 훈련 세트상에서 만들어낸 오차를 **훈련 오차(training error)** 라고 부른다. 그리고 모델이 새로운 샘플 위에서 만들어낸 오차는 **일반화 오차(generalization error)** 라고 부른다.

우리가 원하는 모델은 새로운 샘플 데이터를 대상으로 좋은 성능을 발휘하는 삭습기이다. 이러한 목적을 당성하기 위해 모델은 훈련 데이터에서 모든 데이터의 잠재적인 '보편 규칙'을 찾아내야 하고, 이러한 모델이야말로 새로운 데이터를 만났을 때 정확한 판별과 예측을 제공한다. 그러나 모델이 훈련 데이터에서 학습을 '과도하게 잘하면', 훈련 데이터 중의 일정한 특성을 모든 데이터에서 내재된 일반 성질이라 오해하게 만든다. 즉, 일반화 성능이 떨어진다. 이러한 현상을 **과적합(overfitting)** 이라고 한다. 이와 반대되는 개념으로는 **과소적합(underfitting)**이 있다. 과소적합은 모델이 훈련 데이터의 일반 성질을 제대로 배우지 못했다는 뜻이다.

현실에서 많은 프로젝트를 진행하며 우리는 학습 알고리즘에 대한 다양한 선택지를 제공받는다. 어떠한 경우에는 같은 학습 알고리즘이라 할지라도 파라미터에 따라 다른 모델로 불리기도 한다. 그러면 우리는 어떠한 학습 알고리즘을 사용해야 하고, 어떤 파라미터를 선택해야 할까? 이러한 문제는 머신러닝에서 **모델 선택(model selection)**문제라고 부른다. 이상적인 해답은 당연히 일반화 오차를 기준으로 평가한 뒤, 일반화 오차가 가장 작은 모델을 선택하는 것이다. 그러나 우리는 일반화 오차를 직접적으로 얻을 수 없다. 그렇다면 현실에서는 어떻게 모델을 평가하고 선택하는 것이 이상적일까?

## 2.2 평가 방법

일반적으로 우리는 테스트라는 과정을 통해 모델의 일반화 오차에 대해 평가를 진행하고 모델을 선택한다. 이 과정에서 **테스트 세트(test set)**를 활용하여 학습기가 만나보지 못했던 새로운 샘플에서 어떻게 작동할지 에측할 수 있고, 테스트 세트에서 나온 **테스트 오차(test error)**를 실제 일반화 오차의 근삿값으로 생각한다. 이렇게 생각할 수 있는 이유는 테스트 샘플이 실제 샘플과 동일한 분포를 나타내고 있다고 가정하기 때문이다. 하지만 주의해야 할 것은 테스트 세트와 훈련 세트의 중복을 최대한 피해야 한다는 점이다. 우리는 어떻게 훈련하고 테스트해야 할까? 해답은 데이터 세트 D를 적절히 처리하여 훈련 세트 S와 테스트 세트 T로 나누는 것이다.

### 2.2.1 홀드아웃(hold-out)

**홀드아웃(hold-out)** 방법(검증 세트 기법)은 데이터 세트 D를 겹치지 않는 임의의 두 집합으로 나눈다. 즉,

$$D = S \cup T, S \cap T = \phi.$$

훈련 세트 S를 통해 훈련된 모델은 테스트 세트 T를 활용해 오차를 측정하고 일반화 오차에 대한 추정치를 제공한다.

주의해야 할 것은 훈련/테스트 세트를 나눌 때 되도록이면 데이터 분포가 같게 나눠야 한다는 것이다.  그렇지 않으면 데이터 분포의 편향으로 인해 원치 않은 결과를 얻을 수 있다. 만약 샘플링 시각에서 검증 세트 기법을 본다면, 이러한 분류 작업을 **층화 추출법(stratified sampling)**이라고 한다.

### 2.2.2 교차 검증(cross validation)

**교차 검증(cross validation)**은 데이터 세트 D를 k개의 서로소 집합으로 나누는 것으로 시작한다. 또한 나눌때 층화 추출법을 통해 나누게 된다. 그리고 k-1개의 부분집합들을 훈련 세트로 사용하고 나머지 하나의 부분집합을 테스트 세트로 사용한다. 이렇게 하면 k개의 훈련/테스트 세트가 만들어지고, k번의 훈련과 테스트를 거쳐 k개의 테스트 결괏값의 평균을 얻을 수 있다. 이러한 교차 검증을 **k-fold cross validation**이라 부른다.  k는 일반적으로 10으로 두고 검증하며, 5와 20도 많이 쓰인다.

![https://miro.medium.com/max/2400/1*AAwIlHM8TpAVe4l2FihNUQ.png](https://miro.medium.com/max/2400/1*AAwIlHM8TpAVe4l2FihNUQ.png)

5-fold CV

샘플을 나누는 과정에서 생길 수 있는 차별을 최소화 하기 위해 k겸 교차 검증은 일반적으로 p번을 랜덤하게 반복하여 나누어 진행한다. 즉, 최종 평가 결과는 p번의 k겹 교차 검증을 실행한 값의 평균이다. 

만약 m개의 샘플이 있는 데이터 세트 D를 k = m으로 설정하고 교차 검증을 실행한다면, 이러한 교차 검증은 **LOOCV(Leave-One-Out Cross Validation)** 라고 부른다. LOOCV는 샘플 분류 방법에 대한 영향을 받지 않는다. 왜냐하면, m개의 샘플을 분류하는 방법은 m개의 부분집합을 만드는 것밖에는 없기 때문이다.  LOOCV에 사용한 훈련 세트는 원본 데이터 세트와 비교할 때 1개의 샘플밖에 차이가 나질 않기 때문에 대부분 상황에서 LOOCV 방법을 활용한 모델은 모든 데이터 세트 D를 활용하여 훈련한 모델과 매우 비슷한 성능을 보인다.  따라서 편향이 작다는 장점이 있다. 하지만 데이터 세트의 크기가 매우 클 때, 계산량이 많아진다는 단점이 있다.

![https://blog.kakaocdn.net/dn/b23TOZ/btqyCLbTG72/KetS6sKBR3fzBp0dDbSWo0/img.png](https://blog.kakaocdn.net/dn/b23TOZ/btqyCLbTG72/KetS6sKBR3fzBp0dDbSWo0/img.png)

LOOCV

### 2.2.3 부트스트래핑( bootstrapping)

m개의 샘플이 있는 데이터 세트 D를 가정한다면, 우리는 샘플링을 통해 데이터 세트 D'를 만든다. 매번 D에서 샘플 하나를 꺼내 D'에 복사하여 넣는다. 그리고 다시 D로 돌려보낸다. 이러한 과정을 m번 반복 후, 우리는 m개의 샘플이 들어 있는 데이터세트 D'을 얻는다. 당연하게도 D의 샘플 중 일부는 D'에서 반복 출현 하고, 어떤 샘플은 아예 뽑히지 않을 수도 있다. 샘플이 한 번도 뽑히지 않을 확률의 극한은 다음과 같다.

$$\lim_{m \to \infty }(1-{1 \over m})^m = {1 \over e} \approx 0.368.$$

즉, 부트스트래핑을 사용하면 D 중의 36.8%의 샘플은 D'에 들어가지 못한다. 우리는 D'를 훈련 세트로, D - D'를 테스트 세트로 사용한다. 이러한 테스트를 **Out-of-Bag** 예측이라고 한다. 부트스트래핑은 데이터 세트가 비교적 적거나, 훈련/테스트 세트로 분류하기 힘들때 사용하기 좋다. 그러나 편향이 클 수 있으므로 초기ㅏ 데이터량이 충분할 때는 일반적으로 hold-out과 CV를 더 활용한다.

### 2.2.4 파라미터 튜닝과 최종 모델

모델 평가 및 선택 시 학습 알고리즘의 선택뿐만 아니라 알고리즘 파라미터에 대한 설정도 고려해야 한다. 이러한 과정을 **파라미터 튜닝**이라고 한다.

모델 선택과 파라미터 조율을 위해, 테스트 데이터를 활용하여 성능을 측정하기 전에 검사할 수도 있는데, 모델 평가 및 선택 과정에서 쓰는 평가 테스트 데이터 집합은 **검증 세트(validation set)**라고 한다.

## 2.3 모델 성능 측정

성능 측정은 프로젝트 목적을 반영해야 한다. 어떤 모델이 좋은 모델인지 결정하는 것은 알고리즘과 데이터가 아닌 데이터 분석 목적에 달렸다는 것이다. 

예측을 위한 과제에서 데이터 샘플 $D = \{(\textbf{x}_1, y_1), (\textbf{x}_2, y_2), ..., (\textbf{x}_m,y_1)\}$이 있고 $y_i$는 $\textbf{x}_i$의 정답 데이터이다. 만약 학습기 $f$의 성능을 측정하려면 우리는 학습기의 예측 결과인 $f$ 와 정답 데이터인 $y$를 비교해야 한다.

회귀분석에서 가장 자주 사용하는 선능 측정 방법은 **평균 제곱 오차(mean squared error)** 이다. ****

$$E(f ; D)={1\over m} \sum_{i=1}^m(f(x_i)-y_i)^2.$$

### 2.3.1 오차율과 정확도

오차율은 모든 샘플 수에서 잘못 분류한 샘플 수가 차지하는 비율이고, 정확도는 전체 샘플 수에서 정확히 분류한 샘플 수가 차지하는 비율이다. 오차율은 다음과 같이 정의 할 수 있다.

$$E(f ; D)={1 \over m} \sum_{i=1}^m I(f(s_i) \neq y_i).$$

정확도는 다음과 같이 정의할 수 있다.

$$acc(f;D)={1 \over m}\sum_{i=1}^m I(f(x_i)=y_i) =1-E(f;D). $$

### 2.3.2 재현율, 정밀도 그리고 F1 스코어

오차율과 정확도는 자주 사용되지만 모든 문제에 활용되진 못한다. 예를 들어 정보 검색 중에 우리가 일반적으로 확인하고 싶은 것인 '검색된 자료 중 사용자가 관심 있어 할 내용의 비율' 혹은 '사용자가 좋아하는 내용을 검색하여 제공할 수 있는 확률'등이 있다. **정밀도(precision)**와 **재현율(recall)**은 이러한 요구에 맞는 성능 측도이다.

이진 분류 문제에서 실제 클래스와 학습기가 예측 분류한 클레스의 조합은 **실제 양성(true positive), 거짓 양성(false positive), 실제 음성(true negative), 거짓 음성(false negative)** 총 4가지 형태로 요약된다. 이러한 분류 결과는 **confusion matrix**라고 부른다.

![https://wikidocs.net/images/page/73307/%ED%81%B4%EB%9E%98%EC%8A%A4%EB%B6%88%EA%B7%A0%ED%98%95%EB%AC%B8%EC%A0%9C_%EB%AC%B8%EC%A0%9C%EC%86%8C%EA%B0%9C_%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%AC%EA%B5%AC%EC%A1%B0.jpg](https://wikidocs.net/images/page/73307/%ED%81%B4%EB%9E%98%EC%8A%A4%EB%B6%88%EA%B7%A0%ED%98%95%EB%AC%B8%EC%A0%9C_%EB%AC%B8%EC%A0%9C%EC%86%8C%EA%B0%9C_%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%AC%EA%B5%AC%EC%A1%B0.jpg)

정밀도 P와 재현율 R은 다음과 같이 정의된다.

$$P={TP \over TP+FP},  R ={TP \over TP+FN}$$

정밀도와 재현율 사이에는 트레이드오프가 존재한다. 따라서 두 모델의 성능을 비교할 때 어려울 수 있다. 그래서 정밀도, 재현율을 종합적으로 고려해 비교할 수 있는 성능 측도를 개발한다. F1스코어가 이중 하나이다. 

$$F1={2 \times P \times R \over P+R}={2 \times TP \over 총 샘플 수 + TP-TN}$$

F1스코어는 재현율과 정밀도의 조화 평균이다.  다양한 응용 환경에서 정밀도와 재현율의 중요도는 다를 수밖에 없다. 이럴때 F1스코어를 사용하면 편리하다.

### 2.3.3 ROC와 AUC

ROC 곡선은 이진 분류에서 널리 사용되는 도구이다. 모델의 예측 결과를 기반으로 샘플에 대한 순서를 매기고, 해당 순서에 따라 샘플이 양성값이 될 확률을 계산한다. 그리고 TPR과 FPR값을 계산하여 $x$축과 $y$축에 그려 넣으면 ROC곡선이 완성된다. ROC곡선은 $x$축에는 참 양성률(TPR)을, $y$축에는 거짓 양성률(FPR)을 놓는다.

$$TPR={TP \over TP + FN}, FPR={FP \over TN + FP}$$

![https://i0.wp.com/www.dodomira.com/wp-content/uploads/2016/02/%ED%94%84%EB%A0%88%EC%A0%A0%ED%85%8C%EC%9D%B4%EC%85%981.png?fit=660%2C495](https://i0.wp.com/www.dodomira.com/wp-content/uploads/2016/02/%ED%94%84%EB%A0%88%EC%A0%A0%ED%85%8C%EC%9D%B4%EC%85%981.png?fit=660%2C495)

위 그림에서 1번은 '랜덤 예측 모델'을 나타낸 것이다.  그리고 좌표 위의 점 (0, 1)은 모둔 양성값을 분류해 낸 '가장 이상적인 모델'일 것이다. 현실에서는 테스트 샘플의 개수가 많지 않을 때가 많다. 이렇게 제한된 데이터를 이용해 ROC 그래프를 그리면 위 그래프의 가운데곡선처럼 나올 것이다. 

모델을 비교할 때 어떤 한 모델의 ROC 곡선이 다른 하나의 곡선에 완전히 '포함'되는 경우, 후자가 전자보다 우수한 모델이라고 판단할 수 있다. 하지만 두 모델 ROC곡선에 교차가 발생한다면, 우열을 가리기 힘들다. 이런 상황에서 비교작 합리적인 판단 방법은 ROC 곡선 아래의 면적을 비교하는 것이다. 이를 **AUC(Area Under ROC Curve)**라고 한다.